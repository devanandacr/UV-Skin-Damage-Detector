{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df28955",
   "metadata": {},
   "source": [
    "SUNBURN CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Unzip the dataset\n",
    "# ðŸ” Replace the path below with the exact path to your ZIP file inside your Google Drive\n",
    "!unzip -o '/content/drive/MyDrive/HAM 10000.zip' -d '/content/HAM10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents of the unzipped folder\n",
    "import os\n",
    "\n",
    "root_path = \"/content/HAM10000\"\n",
    "print(\"Contents:\", os.listdir(root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb012e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the metadata CSV\n",
    "df = pd.read_csv('/content/HAM10000/HAM10000_metadata.csv')\n",
    "\n",
    "# Now this will work\n",
    "print(\"Columns in the metadata:\", df.columns)\n",
    "print(\"\\nFirst few rows of metadata:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List the contents of the HAM10000 folder\n",
    "extracted_files = os.listdir('/content/HAM10000')\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2394566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the image directories for both parts\n",
    "image_dir_1 = '/content/HAM10000/HAM10000_images_part_1'\n",
    "image_dir_2 = '/content/HAM10000/HAM10000_images_part_2'\n",
    "\n",
    "# List the image files in both parts\n",
    "image_files_1 = os.listdir(image_dir_1)\n",
    "image_files_2 = os.listdir(image_dir_2)\n",
    "\n",
    "# Combine the lists of image files from both directories\n",
    "image_files = image_files_1 + image_files_2\n",
    "\n",
    "# Check the first 10 image files\n",
    "print(image_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4354cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Select a sample image from the list\n",
    "sample_image = image_files[0]\n",
    "\n",
    "# Build the full path for the selected image\n",
    "image_path = os.path.join(image_dir_1, sample_image) if sample_image in image_files_1 else os.path.join(image_dir_2, sample_image)\n",
    "\n",
    "# Read and display the image\n",
    "img = mpimg.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 1\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Image File Mapping ---\n",
    "# Collect all image file names\n",
    "image_files_1 = os.listdir('/content/HAM10000/HAM10000_images_part_1')\n",
    "image_files_2 = os.listdir('/content/HAM10000/HAM10000_images_part_2')\n",
    "\n",
    "# Map image_id to full path\n",
    "image_id_to_path = {}\n",
    "for filename in image_files_1:\n",
    "    if filename.endswith('.jpg'):\n",
    "        image_id = filename.split('.')[0]\n",
    "        image_id_to_path[image_id] = os.path.join('/content/HAM10000/HAM10000_images_part_1', filename)\n",
    "for filename in image_files_2:\n",
    "    if filename.endswith('.jpg'):\n",
    "        image_id = filename.split('.')[0]\n",
    "        image_id_to_path[image_id] = os.path.join('/content/HAM10000/HAM10000_images_part_2', filename)\n",
    "\n",
    "# --- Load Metadata ---\n",
    "df = pd.read_csv('/content/HAM10000/HAM10000_metadata.csv')\n",
    "df = df[df['image_id'].isin(image_id_to_path.keys())]\n",
    "\n",
    "# --- Optional: Sunburn Severity Mapping ---\n",
    "# You can define this mapping with proper references from medical literature\n",
    "sunburn_mapping = {\n",
    "    'nv': 'no_sunburn',          # Melanocytic nevi\n",
    "    'mel': 'moderate_sunburn',   # Melanoma (linked with UV)\n",
    "    'bkl': 'mild_sunburn',       # Benign keratosis (from chronic sun)\n",
    "    'akiec': 'severe_sunburn',   # Actinic keratoses (UV-induced pre-cancer)\n",
    "    'bcc': 'moderate_sunburn',   # Basal cell carcinoma (from sun)\n",
    "    'vasc': 'no_sunburn',        # Vascular lesions\n",
    "    'df': 'no_sunburn'           # Dermatofibroma\n",
    "}\n",
    "\n",
    "df['sunburn_severity'] = df['dx'].map(sunburn_mapping)\n",
    "\n",
    "# Encode severity labels\n",
    "le_severity = LabelEncoder()\n",
    "df['severity_label'] = le_severity.fit_transform(df['sunburn_severity'])\n",
    "\n",
    "# --- Load and preprocess images ---\n",
    "image_size = (128, 128)\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "print(\"Loading and processing images with severity mapping...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    image_id = row['image_id']\n",
    "    label = row['severity_label']\n",
    "    image_path = image_id_to_path[image_id]\n",
    "\n",
    "    img = load_img(image_path, target_size=image_size)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "\n",
    "    images.append(img_array)\n",
    "    labels.append(label)\n",
    "\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "# --- Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Summary ---\n",
    "print(f\"\\nâœ… Data preparation complete with sunburn severity mapping:\")\n",
    "print(f\"Total images: {len(X)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Image shape: {X_train[0].shape}\")\n",
    "print(f\"Unique severity labels: {np.unique(y)} => {le_severity.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77691e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase_ 2: Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2, EfficientNetB0\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf04668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase_2: Image Data Generators\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_dir = '/content/HAM10000/sunburn_data/train'\n",
    "val_dir = '/content/HAM10000/sunburn_data/val'\n",
    "\n",
    "datagens = {\n",
    "    \"vgg16\": ImageDataGenerator(preprocessing_function=vgg_preprocess),\n",
    "    \"mobilenetv2\": ImageDataGenerator(preprocessing_function=mobilenet_preprocess),\n",
    "    \"efficientnetb0\": ImageDataGenerator(preprocessing_function=efficientnet_preprocess),\n",
    "}\n",
    "\n",
    "def create_data_generators(preprocessor):\n",
    "    return (\n",
    "        datagens[preprocessor].flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical'),\n",
    "        datagens[preprocessor].flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase_2: Model Builder Function\n",
    "def build_model(base_model, input_shape=(224, 224, 3), num_classes=4):\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98318e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase_2: Training Function\n",
    "def train_model(model, train_gen, val_gen, name):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=10,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    val_preds = model.predict(val_gen)\n",
    "    y_true = val_gen.classes\n",
    "    y_pred = np.argmax(val_preds, axis=1)\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=val_gen.class_indices.keys()))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=val_gen.class_indices.keys(), yticklabels=val_gen.class_indices.keys(), cmap='Blues')\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f57df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create directories\n",
    "base_dir = \"/content/HAM10000/sunburn_data\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    for label in df['sunburn_severity'].unique():\n",
    "        os.makedirs(os.path.join(base_dir, split, label), exist_ok=True)\n",
    "\n",
    "# Split metadata\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['sunburn_severity'], random_state=42)\n",
    "\n",
    "# Copy files to respective folders\n",
    "def copy_images(dataframe, split):\n",
    "    for _, row in dataframe.iterrows():\n",
    "        image_id = row['image_id'] + \".jpg\"\n",
    "        label = row['sunburn_severity']\n",
    "        src_path = os.path.join(image_dir_1, image_id) if image_id in image_files_1 else os.path.join(image_dir_2, image_id)\n",
    "        dst_path = os.path.join(base_dir, split, label, image_id)\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "copy_images(train_df, \"train\")\n",
    "copy_images(val_df, \"val\")\n",
    "\n",
    "print(\"âœ… Dataset split complete. Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60397d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create directories\n",
    "base_dir = \"/content/HAM10000/sunburn_data\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    for label in df['sunburn_severity'].unique():\n",
    "        os.makedirs(os.path.join(base_dir, split, label), exist_ok=True)\n",
    "\n",
    "# Split metadata\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['sunburn_severity'], random_state=42)\n",
    "\n",
    "# Copy files to respective folders\n",
    "def copy_images(dataframe, split):\n",
    "    for _, row in dataframe.iterrows():\n",
    "        image_id = row['image_id'] + \".jpg\"\n",
    "        label = row['sunburn_severity']\n",
    "        src_path = os.path.join(image_dir_1, image_id) if image_id in image_files_1 else os.path.join(image_dir_2, image_id)\n",
    "        dst_path = os.path.join(base_dir, split, label, image_id)\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "copy_images(train_df, \"train\")\n",
    "copy_images(val_df, \"val\")\n",
    "\n",
    "print(\"âœ… Dataset split complete. Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and preprocessing keys\n",
    "models_to_train = {\n",
    "    \"VGG16\": (VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)), \"vgg16\"),\n",
    "    \"MobileNetV2\": (MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)), \"mobilenetv2\"),\n",
    "    \"EfficientNetB0\": (EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)), \"efficientnetb0\")\n",
    "}\n",
    "\n",
    "# Loop through each model for training\n",
    "for name, (base_model, preprocess_key) in models_to_train.items():\n",
    "    print(f\"\\nðŸ”¥ Now Training {name} Model\")\n",
    "\n",
    "    # Create generators\n",
    "    train_gen, val_gen = create_data_generators(preprocess_key)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = build_model(base_model)\n",
    "\n",
    "    # Train and evaluate\n",
    "    history = train_model(model, train_gen, val_gen, name)\n",
    "\n",
    "    # Optional: Save the trained model\n",
    "    model.save(f\"{name}_sunburn_model.h5\")\n",
    "    print(f\"âœ… Saved {name} model to disk.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
